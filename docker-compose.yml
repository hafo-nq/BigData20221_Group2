version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - ./hadoop-namenode/name:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    networks:
      - my-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    volumes:
      - ./hadoop-datanode/data:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - 9864:9864
    env_file:
      - ./hadoop.env
    networks:
      - my-network

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
    container_name: elasticsearch
    restart: always
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ./elasticsearch/data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    environment:
      - network.host=elasticsearch
      - discovery.type=single-node
    networks:
      - my-network

  spark-master:
    build: 
      context: ./spark-master
      dockerfile: Dockerfile
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
      - elasticsearch
    volumes:
      - ./spark-master/content:/content
    ports:
      - 4040:4040
      - 8080:8080
      - 7077:7077
      - 8888:8888
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - my-network

  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - my-network
  
  kibana:
    image: docker.elastic.co/kibana/kibana:7.16.2
    container_name: kibana
    depends_on:
      - elasticsearch
    restart: always
    volumes:
      - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml
    ports:
      - 5601:5601
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - my-network

  flask-webapp:
    build:
      context: ./flask-webapp
      dockerfile: Dockerfile
    container_name: flask-webapp
    depends_on:
      - elasticsearch
    volumes:
      - ./flask-webapp/app:/app
    ports:
      - 5000:5000
    networks:
      - my-network


networks:
  my-network:
    name: my-network
    driver: bridge


# /spark/bin/spark-submit /content/crawl.py --master spark://spark-master:7077 --deploy-mode cluster
# /spark/bin/spark-submit /content/machine-learning.py --master spark://spark-master:7077 --deploy-mode cluster